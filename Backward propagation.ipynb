{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t--JP6tAjUTV"
   },
   "source": [
    "#Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JABTor_ujZjG"
   },
   "source": [
    "This function aims to find the updated weights for a three layers simple neuron network model. The input and output values are between 0 and 1. The three layers include an input layer, a hidden layer, and an output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb-8EQOtkIlL"
   },
   "source": [
    "What we known are:\n",
    "\n",
    "\n",
    "1. **Target output** (a list)\n",
    "\n",
    "2. **Current output** (a list)\n",
    "\n",
    "3. **Neuron output** (a list, start from the first input layer neuron and end at the last hidden layer neuron)\n",
    "\n",
    "4. **Current weights** (a list, start from the all weights of the connection betweeen the first input neuron and the hidden neurons, end at all the connections between the last hidden neuron and the output neurons)\n",
    "\n",
    "5. **Learning rate** (a float number between 0 to 1, default to 0.3)\n",
    "\n",
    "6. **Neuron numbers** of input layer (an integer default to 2)\n",
    "\n",
    "7. **Neuron numbers** of hidden layer (an integer default to 3)\n",
    "\n",
    "8. **Neuron numbers** of output layer (an automatically counted integer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can simply run in colab :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/AlexChengCY/Public/blob/main/Backward%20propagation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EPECEvuxrm59"
   },
   "outputs": [],
   "source": [
    "def backpropweight(to,co,no,cw,lr=0.3,nni=2,nnh=3):\n",
    "  '''\n",
    "  Variables:\n",
    "  to = Target output\n",
    "  co = Current output\n",
    "  no = Neuron output\n",
    "  cw = Current weights\n",
    "  lr = Learning rate\n",
    "  nni = Neuron numbers of input layer\n",
    "  nnh = Neuron numbers of hidden layer\n",
    "  nno = Neuron numbers of output layer\n",
    "  uw = Updated weights\n",
    "  lte_out = a list storing learning rate times errors of the output layer neurons\n",
    "  lte_hid = a list storing learning rate times errors of the hidden layer neurons\n",
    "  wf2 = Weight numbers connecting the input and hidden layers\n",
    "  wl2 = Weight numbers connecting the hidden and output layers\n",
    "  '''\n",
    "  #Make sure the neuron numbers of the current output and the neuron output are the same\n",
    "  if len(to)!=len(co):\n",
    "    print(\"The neuron numbers of the current output and the neuron output must be the same.\")\n",
    "    return None\n",
    "  #Set variables\n",
    "  nno = len(to)\n",
    "  wf2 = nni*nnh\n",
    "  wl2 = nnh*nno\n",
    "  uw = []\n",
    "  for i in range(wf2+wl2):\n",
    "    uw.append(0)\n",
    "  for i in range(wf2+wl2-len(cw)):\n",
    "    cw.append(0)\n",
    "  #Find the learning rate times errors of output layer neurons\n",
    "  lte_out = []\n",
    "  for i in range(nno):\n",
    "    error = (to[i]-co[i])*(co[i])*(1-co[i])\n",
    "    lte_out.append(lr*error)\n",
    "  #Find the updated weights btween the hidden layer and the output layer\n",
    "  for i in range(len(cw[wf2:])):\n",
    "    k = i//nnh\n",
    "    i += wf2\n",
    "    uw[i] = cw[i]+lte_out[i%nno]*no[k]\n",
    "  #Find the learning rate times errors of the hidden layer neurons\n",
    "  lte_hid = []\n",
    "  for i in range(nnh):\n",
    "    k = wf2+i*3 #k is the index for hidden neurons\n",
    "    etw = 0 #etw is the error of the corresponding output neuron times current weight\n",
    "    for j in range(nno):\n",
    "      etw+= lte_out[j]*cw[k+j]\n",
    "    i += nni\n",
    "    error = (no[i])*(1-no[i])*etw\n",
    "    lte_hid.append(lr*error)\n",
    "  #Find the updated weights btween the input layer and the hidden layer\n",
    "  for i in range(wf2):\n",
    "    k = i//nnh\n",
    "    uw[i] = cw[i]+lte_hid[i%nnh]*no[k]\n",
    "  return uw"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Backward propagation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
